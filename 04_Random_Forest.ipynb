{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 랜덤 포레스트 Random Forest\n",
    "## 0. 소개\n",
    "- 강력한 앙상블 학습 방법 중 하나\n",
    "- 여러 개의 결정 트리를 사용해 예측 성능을 향상시키는 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 개념\n",
    "### 1.1 결정 트리 Decision Tree\n",
    "- 결정 트리는 데이터의 특성을 기반으로 의사 결정을 내리는 트리 구조 모델\n",
    "- 분류Classification와 회귀Regression 모두에 사용 가능\n",
    "- 각 노드는 하나의 특성을 기준으로 데이터 분할\n",
    "### 1.2 앙상블 학습 Ensemble Learning\n",
    "- 여러 개의 모델을 결합하여 더 나의 성능을 얻는 방법\n",
    "- 랜덤 포레스트는 앙상블 학습의 일종\n",
    "- 정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘\n",
    "### 1.3 랜덤 포레스트 Random Forest\n",
    "- 여러 개의 결정 트리를 만들어, 각 트리의 예측을 회귀의 경우 평균, 분류의 경우 다수결 투표로 결합하는 방법\n",
    "    1. 부트스트랩 샘플링 Bootstrap Sampling\n",
    "        - 원본 데이터에서 중복을 허용하여 여러 개의 샘플 데이터를 생성\n",
    "    2. 랜덤 특성 선택 Random Feature Selection\n",
    "        - 각 트리의 노드를 분할할 때, 임의로 선택된 일부 특성만 고려\n",
    "- 각 결정 트리의 예측을 사용해 최종 예측을 만듦\n",
    "- 각 노드를 분할할 때 전체 특성 중 일부 특성을 무작위로 골라 최선의 분할을 찾음\n",
    "    - RandomForestClassifier는 전체 특성 개수의 제곱근 만큼 특성 선택, 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측\n",
    "    - RandomForestRegressor는 전체 특성을 사용, 각 트리의 단순 예측을 평균"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 장단점\n",
    "### 장점\n",
    "- 여러 트리를 결합하여 예측 성능을 향상시킴\n",
    "- 랜덤 특성 선택과 부트스트랩 샘플링을 통해 과적합 방지\n",
    "- 분류, 회귀 모두 사용 가능\n",
    "### 단점\n",
    "- 개별 결정 트리는 해석이 쉽지만, 여러 트리를 결합한 랜덤 포레스트는 해석이 어려움\n",
    "- 많은 트리를 학습하고 예측하는 데 시간이 오래 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine  =  pd.read_csv('https://bit.ly/wine-date')\n",
    "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
    "target = wine['class'].to_numpy()\n",
    "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier\n",
    "- 기본적으로 100개의 결정 트리 사용\n",
    "- n_jobs = -1 : 최대한 병렬로 교차 검증 수행\n",
    "- reuturn_train_score = True : 검증 세트의 점수와 훈련 세트의 점수를 비교하여 과적합 파악"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교차 검증 cross-vaildation\n",
    "- 모델의 성능을 평가하기 위해 데이터를 여러 번 나누어 학습, 검증\n",
    "- 단순히 훈련 데이터, 테스트 데이터로 한 번 나누어 평가하는 것 보다 더 신뢰할 수 있는 성능 평가 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9973541965122431 0.8905151032797809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs = -1, random_state=42)\n",
    "scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 훈련 데이터 과대적합 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'alcohol', 'sugar', 'pH' 순서의 특성 중요도 알아보기\n",
    "- 랜덤 포레스트의 특성 중요도는 각 결정 트리의 특성 중요도를 취합한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23167441, 0.50039841, 0.26792718])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(train_input, train_target)\n",
    "rf.feature_importances_"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정 트리의 특성 중요도와 다름\n",
    "- 03_Decision_Tree_impoertances\n",
    "\n",
    "    ![image.png](attachment:image.png)\n",
    "- 랜덤 포레스트가 특성의 일부를 랜덤 선택하여 결정 트리를 훈련하기 때문\n",
    "- 그 결과 한 특성에 과하게 집중하지 않고, 좀 더 다양한 특성이 훈련에 기역\n",
    "- 과적합을 줄이고 일반화 성능을 높임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier의 다른 기능\n",
    "- 분류 모델은 훈련 데이터에서 중복을 혀용 (부트스트랩 샘플)\n",
    "- 부트스트랩 샘플에 속하지 않은 샘플(OOB;Out Of Bag 샘플)을 이용해 부트스랩 샘플로 훈련한 결정 트리를 평가\n",
    "- oob_score = True로 설정\n",
    "    - 랜덤 포레스트는 각 결정 트리의 OOB 점수 평균을 출력\n",
    "- 교차 검증을 대신해 더 많은 샘플을 훈련 데이터로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8934000384837406"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
    "rf.fit(train_input, train_target)\n",
    "rf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 엑스트라 트리 Extra Tree\n",
    "- 랜덤 포레스트와 비슷\n",
    "    - 기본적으로 100개의 결정 트리 훈련\n",
    "    - 결정 트리의 매개변수 대부분을 지원\n",
    "    - 일정 특성을 랜덤 선택하여 노드 분할\n",
    "### 차이점\n",
    "- 부트스트랩 샘플 사용 안함\n",
    "    - 결정 트리를 만들 때 전체 훈련 데이터 사용\n",
    "- 노드 분할 시, 무작위 분할\n",
    "    - 성능 감소\n",
    "    - 많은 트리를 앙상블 하기 때문에 과대적합 방지\n",
    "    - 검증 데이터의 점수를 높이는 효과\n",
    "- splitter = 'random'인 결정 트리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9974503966084433 0.8887848893166506\n"
     ]
    }
   ],
   "source": [
    "# 엑스트라 트리 분류 버전\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
    "scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤 포레스트와 비슷한 결과\n",
    "\n",
    "- 데이터의 특성이 많지 않기 때문\n",
    "- 보통 엑스트라 트리가 무작위성이 더 크기 때문에 램덤 포레스트보다 더 많은 결정 트리를 훈련해야 함\n",
    "### 엑스트라 트리의 장점\n",
    "- 노드를 랜덤 분할하기 때문에 빠른 계산 속도\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20183568, 0.52242907, 0.27573525])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et.fit(train_input, train_target)\n",
    "et.feature_importances_\n",
    "# 엑스트라 트리 분류의 특성 중요도\n",
    "# 'alcohol', 'sugar', 'pH' 순서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 앙상블 학습\n",
    "- 강력하고 뛰어난 성능\n",
    "- 사이킷런에서 랜덤 포레스트, 엑스트라 트리, 그레이디언트 부스팅, 히스토그램 기반 그레이디언트 부스팅 제공\n",
    "### 랜덤 포레스트\n",
    "- 가장 대표적인 앙상블 학습 알고리즘\n",
    "- 성능이 좋고 안정적\n",
    "- 결정 트리를 훈련하기 위해 부트스트랩 샘플을 만들고 전체 특성 중 일부 랜덤 선택하여 결정 트리 만듬\n",
    "### 엑스트라 트리\n",
    "- 랜덤 포레스트와 비슷하지만 부트스트랩 샘플 사용 안함\n",
    "    - 노드를 분할할때 가장 좋은 분할이 아니라 랜덤 분할\n",
    "- 훈련 속도가 빠르지만, 보통 더 많은 트리가 필요\n",
    "### 그레이디언트 부스팅\n",
    "- 깊이가 얕은 트리를 연속 추가하여 손실 함수를 최소화하는 앙상블 학습\n",
    "- 성능이 뛰어남\n",
    "- 병렬로 훈련할 수 없음\n",
    "    - 랜덤 포레스트나 엑스트라 트리보다 훈련 속도가 느림\n",
    "- 학습률 매개변수를 이용해 모델의 복잡도 제어\n",
    "    - 학습률 매개변수가 크면 복잡하고 훈련 데이터에 과대적합된 결과 모델을 얻을 수 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
